## AN AUDIO-TEXTUAL DIFFUSION MODEL FOR CONVERTING SPEECH SIGNALS INTO ULTRASOUND TONGUE IMAGING DATA

Acoustic-to-articulatory inversion (AAI) is to convert audio into articulator movements, such as ultrasound tongue imaging (UTI) data. An issue of existing AAI methods is only using the personalized acoustic information to derive the general patterns of tongue motions, and thus the quality of generated UTI data is limited. To address this issue, this paper proposes an audio-textual diffusion model for the UTI data generation task. In this model, the inherent acoustic characteristics of individuals related to the tongue motion details are encoded by using wav2vec 2.0, while the ASR transcriptions related to the universality of tongue motions are encoded by using BERT. UTI data are then generated by using a diffusion module. Experimental results showed that the proposed diffusion model could generate high-quality UTI data with clear tongue contour that is crucial for the linguistic analysis and clinical assessment.	

in this paper, All of our metrics are SOTA！

## DATASET preprocess for Ultrasound  😀

Our ultrasound data mainly consist of parallel ultrasound and speech data, for which we perform the following preprocessing steps:

(1) Ultrasound duration check   (2) Ultrasound video check   (3) Audio-ultrasound data registration

The frame rate of ultrasound collection is 60fps. The checks involve manual and code-based screenings. The code is located under Utils. The ultrasound data used has been processed and does not require secondary preprocessing.

## Acquire the DATASET 😀!

Our ultrasound data is collected from Shenzhen, China. It includes 44 healthy adults, and the database size is continuously expanding. Currently, the database has a size of 6.89 hours, comprising approximately 7000 ultrasound files, including 21 males and 23 females. For more details regarding the usage of the database, you can click on this link to apply: 

The data collection has undergone ethical review and obtained informed consent from the participants, considering the ethical and privacy concerns of the subjects involved. Please refrain from unauthorized dissemination to protect the privacy of the participants.

Sample：

![image-20230915155021826](C:\Users\杨毓栋\AppData\Roaming\Typora\typora-user-images\image-20230915155021826.png)



## Infer Config 

- To setup this repository, first clone it and cd into it

  ```
  git clone https://github.com/yangyudong2020/Wav2uti.git
  cd Wav2uti
  ```

- Setup a new conda env: 

  ```
  conda create -n wav2uti python=3.10 -y
  conda activate echodiff
  ```

- Yours Datasets

```
Dataset
├───Ultrasound
│   ├───ult
│   │   |   video001.ult
│   │   │   video002.ult
│   │   │   video003.ult
│   │   │   ...
│   └───wav
│   │   |   wav001.wav
│   │   │   wav002.wav
│   │   │   wav003.wav
│   │   │   ...
```

我们将在审稿结束后公开我们的全部推理方法与训练过程，目前提供测试流程

```shell
pip install -r requestments.txt
./ init.sh
```

准备您的预训练权重代码，我们遵循Bert和Wav2Vec2.0的设定，它们的huggingface🤗在这里：[Wav2Vec2.0](https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn) and [Bert](https://huggingface.co/bert-base-chinese)

我们需要使用这两个预训练权重，并进行个性化语音信息与普适性语音信息的提取

进行后续训练需要需要传递如下参数：

| Config             | Is_True?            |
| ------------------ | ------------------- |
| --dataset_path     | True                |
| --image_sizes      | True                |
| --embed_dim        | True                |
| --num_sample_steps | True                |
| --sigma_min        | False defalut 0.001 |
| --sigma_max        | False defalut 160   |
| --sigma_data       | False defalut 0.25  |
| --rho              | False defalut 7     |
| Pmean              | False defalut -1.2  |
| Pstd               | False defalut 1.2   |
| Schurn             | False defalut 80    |
| Stmin              | False defalut 0.05  |
| Stmax              | False defalut 50    |
| Snoise             | False defalut 1.003 |

测试代码使用：

```shell
python infer.py --dataset_path=path --image_sizes=112 --embed_dim=256 --num_sample_steps=16
```

## Evaluation Metrics 

In this paper, The Metrics is RMSE、LPIPS、FID and PSNR

Evaluation Metrics code path in evaluation/

FID evaluation work by stylegan-v

fellow this tree:

```
MODEL Output
├───Ultrasound
│   ├───images
│   │   ├───real
│   │   │   ├───video001
│   │   │   │   image001.jpg
│   │   │   │   image002.jpg
│   │   │   ...
│   │   └───fake
│   │       ├───video001
│   │       │   image001.jpg
│   │       │   image002.jpg
│   │       ...
│   └───videos
│       video001.gif
│       video002.gif
│       ...
```

This will populate the  folder with the frames of the videos. Now you can run the FID metric computation with:


```
cd stylegan-v
python src/scripts/calc_metrics_for_dataset.py --real_data_path path/to/images/real --fake_data_path path/to/images/fake --mirror 0 --gpus 1 --resolution 128 --metrics fid50k_full
```



## Result







## 

## Citation

```
Underviewer
```























